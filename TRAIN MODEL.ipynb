{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d37ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8097660223804679\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.43      0.53        54\n",
      "         2.0       0.00      0.00      0.00        14\n",
      "         3.0       0.00      0.00      0.00        25\n",
      "         4.0       0.31      0.05      0.08       110\n",
      "         5.0       0.83      0.98      0.90       780\n",
      "\n",
      "    accuracy                           0.81       983\n",
      "   macro avg       0.37      0.29      0.30       983\n",
      "weighted avg       0.73      0.81      0.75       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#unsing naive bays model in this N-gram feature and sentiment polarity to predict overall(rating or score)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "#DATASET\n",
    "\n",
    "file_path = r'C:\\Users\\nh013\\Desktop\\amazaon dataset\\amazon_reviews.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# DROP ROWS WITH MISSING VALUES\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE DUBLICATE ROWS\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# REMOVE ANY URL\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# REMOVE SPECIAL CHARACTERS\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# CONVERT ALL TEXT TO LOWERCASE\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: x.lower())\n",
    "\n",
    "# REMOVE STOP WORDS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# STEMMING\n",
    "stemmer = PorterStemmer()\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# SENTIMENT ANALYSIS USING VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# BUILD FUNCTION TO GENERATE N GRMAS\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    ngrams_list = list(ngrams(tokenized_text, n))\n",
    "    return [' '.join(grams) for grams in ngrams_list]\n",
    "\n",
    "\n",
    "# N-GRAMS AS A NEW COLUMN\n",
    "n = 2  #  (EXAMPLE n=2 FOR BIGRAMS)\n",
    "df['ngrams'] = df['reviewText'].apply(lambda x: generate_ngrams(x, n=n))\n",
    "df['sentiment_polarity'] = df['reviewText'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# SPLIT DATA\n",
    "X = df['reviewText']\n",
    "y = df['overall']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TRAIN MODEL\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTION\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# EVALUATE THE MODEL ACCURACY AND PERFORMENCE\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de30ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdd9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost model: 0.8107833163784334\n",
      "Predicted Overall Ratings: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5\n",
      " 4 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 4 5 5 5 5 5 5 5 5 5 4 1 5 4 4 5 5 5\n",
      " 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 1 4 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 4 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 4\n",
      " 5 5 5 5 1 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 1 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 4 5 5 5 5 5 5 4 5 5 5 5 5 5 5 1 5 5 5 5\n",
      " 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 5 4 5 5 5 5 5 5 5 5 5 5 5 3 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 1 5 5 5 5 5 5 5 5 5 1 5 5 5 5\n",
      " 5 5 5 1 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 5 5 5 5 5 5 4 5 5 5 5 5 5 5\n",
      " 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 1 5 3 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 1 5 5 4 5 5 5 5 5\n",
      " 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 1 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 1 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 4 5]\n"
     ]
    }
   ],
   "source": [
    "# using XGBOOST model to predict overall rating in sentiment polarity and N-grams feature\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#DATASET\n",
    "\n",
    "file_path = r'C:\\Users\\nh013\\Desktop\\amazaon dataset\\amazon_reviews.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# DROP ROWS WITH MISSING VALUES\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE DUBLICATE ROWS\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# REMOVE ANY URL\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# REMOVE SPECIAL CHARACTERS\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# CONVERT ALL TEXT TO LOWERCASE\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: x.lower())\n",
    "\n",
    "# REMOVE STOP WORDS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# STEMMING\n",
    "stemmer = PorterStemmer()\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# SENTIMENT ANALYSIS USING VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# BUILD FUNCTION TO GENERATE N GRMAS\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    ngrams_list = list(ngrams(tokenized_text, n))\n",
    "    return [' '.join(grams) for grams in ngrams_list]\n",
    "\n",
    "\n",
    "# N-GRAMS AS A NEW COLUMN\n",
    "n = 2  #  (EXAMPLE n=2 FOR BIGRAMS)\n",
    "df['ngrams'] = df['reviewText'].apply(lambda x: generate_ngrams(x, n=n))\n",
    "df['sentiment_polarity'] = df['reviewText'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# SPLIT DATA\n",
    "x = df['reviewText']\n",
    "y = df['overall']\n",
    "\n",
    "# MAP TARGET VARIABLES VALUES START FROM 0\n",
    "y_mapped = y - 1\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_mapped.astype(int), test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# COUNT VECTORIZE OBJECT\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# FIT THE MODEL\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# TTRANSFORM THE TEST DATA\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# XGBOOST CLASSIFIER\n",
    "classifier = xgb.XGBClassifier()\n",
    "\n",
    "# TRAIN  MODEL\n",
    "classifier.fit(x_train_vectorized, y_train)\n",
    "\n",
    "# PREDICT OVERALL RATING ON TEST DATA\n",
    "predicted_ratings = classifier.predict(x_test_vectorized)\n",
    "\n",
    "# ACCURACY SCORE\n",
    "accuracy = accuracy_score(y_test, predicted_ratings)\n",
    "print(\"Accuracy of XGBoost model:\", accuracy)\n",
    "\n",
    "\n",
    "# MAP THE PREDICTED RATING BACK TO THE ORGINAL FORM\n",
    "predicted_ratings_mapped = predicted_ratings + 1\n",
    "\n",
    "\n",
    "print(\"Predicted Overall Ratings:\", predicted_ratings_mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf574db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
