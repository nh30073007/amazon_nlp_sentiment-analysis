{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73633574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       purchas devic work advertis never much phone m...\n",
      "2       work expect sprung higher capac think made bit...\n",
      "3       think work greathad diff bran 64gb card went s...\n",
      "4       bought retail packag arriv legit orang envelop...\n",
      "5       mini storag doesnt anyth els suppos purchas ad...\n",
      "                              ...                        \n",
      "4910    bought sandisk 16gb class 10 use htc inspir 3 ...\n",
      "4911    use extend capabl samsung galaxi note 10 great...\n",
      "4912    great card fast reliabl come option adapt sd s...\n",
      "4913           good amount space stuff want fit gopro say\n",
      "4914    ive heard bad thing 64gb micro sd card crap we...\n",
      "Name: reviewText, Length: 4913, dtype: object\n",
      "Predicted Positive Word Counts: [ 1  2  1  1  1  1  1  1  2  2  2  2  1  1  1  2  1  2  1  1  3  1  1  1\n",
      "  2  1  1  1  1  2  2  2  1  2  1  1  1  1  1  3  1  1  1  2  3  2  1  1\n",
      "  2  1  2  2  2  1  1  1  2  2  2  1  1  1  2  3  1  1  2  1  1  1  2  2\n",
      "  2  1 13  2  2  2  2  1  2  1  1  2  2  1  2  1  1  2  2  1  2  3  1  2\n",
      "  1  2  2  1  1  1  2  2  2  2  1  2  1  1  2  2  1  2  2  1  2  1  1  2\n",
      "  1  2  1  1  1  1  1  1  2  2  0  1  2  2  1  2  2  2  1  1  1  2  2  2\n",
      "  1  1  2  1  1  2  3  1  1  2  1  3  2  1  2  1  2  1  2  2  1  1  1  2\n",
      "  1  2  0  2  1  1  1  2  1  1  1  1  1  2  1  1  1  1  2  1  2  2  1  2\n",
      "  1  1  3  2  3  1  2  2  2  1  1  2  1  2  1  3  1  1  2  2  1  1  1  1\n",
      "  1  2  2  1  1  2  2  1  1  1  3  2  3  2  2  2  0  0  2  1  1  1  1  1\n",
      "  1  1  1  2  2  0  2  2  2  1  1  1  1  1  1  2  2  2  1  0  1  2  1  1\n",
      "  1  1  2  2  2  1  1  2  1  3  1  1  1  1  2  2  1  2  2  2  2  1  1  2\n",
      "  2  1  1  2  0  2  2  1  2  2  1  1  1  2  2  2  1  2  0  1  2  2  1  2\n",
      "  1  2  2  2  1  1  1  1  1  0  1  2  1  1  0  2  1  2  2  1  1  2  2  2\n",
      "  2  1  2  1  2  2  2  1  1  1  2  1  2  2  2  1  2  2  3  1  2  2  1  1\n",
      "  2  2  2  2  1  2  2  2  2  2  1  1  1  3  2  2  1  2  2  2  2  2  1  2\n",
      "  1  2  1  2  2  1  1  4  1  1  2  2  2  1  1  1  1  1  2  1  1  2  2  1\n",
      "  2  2  1  2  2  2  1  1  1  2  2  2  2  1  1  1  2  2  1  1  2  1  2  2\n",
      "  1  1  2  1  2  1  2  1  1  2  1  2  2  1  2  2  2  1  1  1  1  1  1  1\n",
      "  2  1  1  2  2  2  2  1  2  1  1  2  2  2  2  2  2  1  1  2  3  1  1  1\n",
      "  2  2  1  2  2  3  1  1  1  1  3  2  2  2  2  1  2  1  2  2  1  1  2  2\n",
      "  2  2  2  2  2  1  1  2  2  2  1  2  2  1  2  2  1  1  2  1  2  0  1  1\n",
      "  2  1  2  2  2  1  1  2  2  1  2  2  1  2  2  2  1  1  1  2  2  3  1  2\n",
      "  1  2  1  2  2  2  2  1  2  2  2  1  2  1  1  2  1  2  2  2  1  3  1  2\n",
      "  1  2  2  1  1  2  2  1  2  1  2  1  2  2  1  1  2  1  2  1  1  3  2  2\n",
      "  1  1  1  1  1  1  1  1  2  2  1  1  1  2  3  2  2  2  1  2  1  2  1  2\n",
      "  1  2  1  1  1  2  2  2  2  2  2  2  1  1  2  1  1  2  5  2  2  1  1  1\n",
      "  2  2  2  1  1  1  1  1  1  1  2  1  1  1  1  1  2  2  2  2  2  2  1  1\n",
      "  2  3  1  2  1  1  1  1  2  2  1  2  2  2  3  1  1  1  2  2  3  1  1  2\n",
      "  1  2  1  1  1  1  2  1  3  1  1  3  1  2  2  2  1  2  2  1  2  3  1  2\n",
      "  1  2  2  2  4  2  1  1  2  2  1  2  2  1  1  1  3  1  1  1  2  3  2  2\n",
      "  2  3  3  1  2  1  1  2  2  2  1  1  1  2  2  1  1  2  2  1  2  2  1  1\n",
      "  2  1  1  1  1  3  1  1  2  1  1  2  1  1  1  1  2  2  1  1  1  1  2  1\n",
      "  1  2  1  1  2  1  1  1  2  2  1  2  1  2  2  1  1  2  2  2  1  3  1  1\n",
      "  2  2  2  1  1  1  2  2  2  2  1  3  2  1  2  1  1  2  1  2  2  2  1  2\n",
      "  1  1  2  2  2  2  1  2  2  1  1  2  2  1  2  2  2  2  1  2  2  2  1  3\n",
      "  1  1  1  2  2  2  3  2  1  2  2  2  2  2  2  2  1  1  2  1  2  2  2  1\n",
      "  1  1  2  2  2  1  2  1  1  2  1  2  2  2  1  2  1  1  2  1  2  2  2  3\n",
      "  2  1  2  2  2  1  2  1  1  2  2  1  2  2  2  1  2  2  1  2  1  1  1  1\n",
      "  2  1  1  2  1  2  1  2  1  1  2  2  2  2  1  2  1  1  1  2  1  2  2  1\n",
      "  1  1  2  2  1  1  0  1  1  1  1  2  1  2  2  2  1  2  2  1  3  1  2]\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes classifier and a count vectorizer in possitive negative word count feature\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "\n",
    "# Set the file path to your dataset\n",
    "file_path = r'C:\\Users\\nh013\\Desktop\\amazaon dataset\\amazon_reviews.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove any rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove any URLs\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# Remove any special characters\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Convert all text to lowercase\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Apply stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Positive and Negative word counts\n",
    "positive_count = []\n",
    "negative_count = []\n",
    "\n",
    "# Perform sentiment analysis using VADER\n",
    "for index, row in df.iterrows():\n",
    "    review = row['reviewText']\n",
    "    sentiment_scores = sia.polarity_scores(review)\n",
    "    positive_count.append(sum([1 for word in review.split() if word in sia.lexicon and sia.lexicon[word] > 0]))\n",
    "    negative_count.append(sum([1 for word in review.split() if word in sia.lexicon and sia.lexicon[word] < 0]))\n",
    "\n",
    "  # Print the preprocessed reviewText column\n",
    "print(df['reviewText'])\n",
    "\n",
    "\n",
    "# Add positive and negative word count columns to the DataFrame\n",
    "df['Positive Word Count'] = positive_count\n",
    "df['Negative Word Count'] = negative_count\n",
    "\n",
    "# Create a count vectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviewText'], df['Positive Word Count'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the vectorizer to the training data and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Predict the positive word counts for the test data\n",
    "predicted_counts = classifier.predict(X_test_vectorized)\n",
    "\n",
    "\n",
    "# Print the predicted positive word counts\n",
    "print(\"Predicted Positive Word Counts:\", predicted_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fe8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d1e3ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       purchas devic work advertis never much phone m...\n",
      "2       work expect sprung higher capac think made bit...\n",
      "3       think work greathad diff bran 64gb card went s...\n",
      "4       bought retail packag arriv legit orang envelop...\n",
      "5       mini storag doesnt anyth els suppos purchas ad...\n",
      "                              ...                        \n",
      "4910    bought sandisk 16gb class 10 use htc inspir 3 ...\n",
      "4911    use extend capabl samsung galaxi note 10 great...\n",
      "4912    great card fast reliabl come option adapt sd s...\n",
      "4913           good amount space stuff want fit gopro say\n",
      "4914    ive heard bad thing 64gb micro sd card crap we...\n",
      "Name: reviewText, Length: 4913, dtype: object\n",
      "Accuracy of Naive Bayes model: 0.31943031536113936\n",
      "Predicted Positive Word Counts: [ 1  2  1  1  1  1  1  1  2  2  2  2  1  1  1  2  1  2  1  1  3  1  1  1\n",
      "  2  1  1  1  1  2  2  2  1  2  1  1  1  1  1  3  1  1  1  2  3  2  1  1\n",
      "  2  1  2  2  2  1  1  1  2  2  2  1  1  1  2  3  1  1  2  1  1  1  2  2\n",
      "  2  1 13  2  2  2  2  1  2  1  1  2  2  1  2  1  1  2  2  1  2  3  1  2\n",
      "  1  2  2  1  1  1  2  2  2  2  1  2  1  1  2  2  1  2  2  1  2  1  1  2\n",
      "  1  2  1  1  1  1  1  1  2  2  0  1  2  2  1  2  2  2  1  1  1  2  2  2\n",
      "  1  1  2  1  1  2  3  1  1  2  1  3  2  1  2  1  2  1  2  2  1  1  1  2\n",
      "  1  2  0  2  1  1  1  2  1  1  1  1  1  2  1  1  1  1  2  1  2  2  1  2\n",
      "  1  1  3  2  3  1  2  2  2  1  1  2  1  2  1  3  1  1  2  2  1  1  1  1\n",
      "  1  2  2  1  1  2  2  1  1  1  3  2  3  2  2  2  0  0  2  1  1  1  1  1\n",
      "  1  1  1  2  2  0  2  2  2  1  1  1  1  1  1  2  2  2  1  0  1  2  1  1\n",
      "  1  1  2  2  2  1  1  2  1  3  1  1  1  1  2  2  1  2  2  2  2  1  1  2\n",
      "  2  1  1  2  0  2  2  1  2  2  1  1  1  2  2  2  1  2  0  1  2  2  1  2\n",
      "  1  2  2  2  1  1  1  1  1  0  1  2  1  1  0  2  1  2  2  1  1  2  2  2\n",
      "  2  1  2  1  2  2  2  1  1  1  2  1  2  2  2  1  2  2  3  1  2  2  1  1\n",
      "  2  2  2  2  1  2  2  2  2  2  1  1  1  3  2  2  1  2  2  2  2  2  1  2\n",
      "  1  2  1  2  2  1  1  4  1  1  2  2  2  1  1  1  1  1  2  1  1  2  2  1\n",
      "  2  2  1  2  2  2  1  1  1  2  2  2  2  1  1  1  2  2  1  1  2  1  2  2\n",
      "  1  1  2  1  2  1  2  1  1  2  1  2  2  1  2  2  2  1  1  1  1  1  1  1\n",
      "  2  1  1  2  2  2  2  1  2  1  1  2  2  2  2  2  2  1  1  2  3  1  1  1\n",
      "  2  2  1  2  2  3  1  1  1  1  3  2  2  2  2  1  2  1  2  2  1  1  2  2\n",
      "  2  2  2  2  2  1  1  2  2  2  1  2  2  1  2  2  1  1  2  1  2  0  1  1\n",
      "  2  1  2  2  2  1  1  2  2  1  2  2  1  2  2  2  1  1  1  2  2  3  1  2\n",
      "  1  2  1  2  2  2  2  1  2  2  2  1  2  1  1  2  1  2  2  2  1  3  1  2\n",
      "  1  2  2  1  1  2  2  1  2  1  2  1  2  2  1  1  2  1  2  1  1  3  2  2\n",
      "  1  1  1  1  1  1  1  1  2  2  1  1  1  2  3  2  2  2  1  2  1  2  1  2\n",
      "  1  2  1  1  1  2  2  2  2  2  2  2  1  1  2  1  1  2  5  2  2  1  1  1\n",
      "  2  2  2  1  1  1  1  1  1  1  2  1  1  1  1  1  2  2  2  2  2  2  1  1\n",
      "  2  3  1  2  1  1  1  1  2  2  1  2  2  2  3  1  1  1  2  2  3  1  1  2\n",
      "  1  2  1  1  1  1  2  1  3  1  1  3  1  2  2  2  1  2  2  1  2  3  1  2\n",
      "  1  2  2  2  4  2  1  1  2  2  1  2  2  1  1  1  3  1  1  1  2  3  2  2\n",
      "  2  3  3  1  2  1  1  2  2  2  1  1  1  2  2  1  1  2  2  1  2  2  1  1\n",
      "  2  1  1  1  1  3  1  1  2  1  1  2  1  1  1  1  2  2  1  1  1  1  2  1\n",
      "  1  2  1  1  2  1  1  1  2  2  1  2  1  2  2  1  1  2  2  2  1  3  1  1\n",
      "  2  2  2  1  1  1  2  2  2  2  1  3  2  1  2  1  1  2  1  2  2  2  1  2\n",
      "  1  1  2  2  2  2  1  2  2  1  1  2  2  1  2  2  2  2  1  2  2  2  1  3\n",
      "  1  1  1  2  2  2  3  2  1  2  2  2  2  2  2  2  1  1  2  1  2  2  2  1\n",
      "  1  1  2  2  2  1  2  1  1  2  1  2  2  2  1  2  1  1  2  1  2  2  2  3\n",
      "  2  1  2  2  2  1  2  1  1  2  2  1  2  2  2  1  2  2  1  2  1  1  1  1\n",
      "  2  1  1  2  1  2  1  2  1  1  2  2  2  2  1  2  1  1  1  2  1  2  2  1\n",
      "  1  1  2  2  1  1  0  1  1  1  1  2  1  2  2  2  1  2  2  1  3  1  2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set the file path to your dataset\n",
    "file_path = r'C:\\Users\\nh013\\Desktop\\amazaon dataset\\amazon_reviews.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove any rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove any URLs\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# Remove any special characters\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Convert all text to lowercase\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Apply stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Positive and Negative word counts\n",
    "positive_count = []\n",
    "negative_count = []\n",
    "\n",
    "# Perform sentiment analysis using VADER\n",
    "for index, row in df.iterrows():\n",
    "    review = row['reviewText']\n",
    "    sentiment_scores = sia.polarity_scores(review)\n",
    "    positive_count.append(sum([1 for word in review.split() if word in sia.lexicon and sia.lexicon[word] > 0]))\n",
    "    negative_count.append(sum([1 for word in review.split() if word in sia.lexicon and sia.lexicon[word] < 0]))\n",
    "\n",
    "# Print the preprocessed reviewText column\n",
    "print(df['reviewText'])\n",
    "\n",
    "# Add positive and negative word count columns to the DataFrame\n",
    "df['Positive Word Count'] = positive_count\n",
    "df['Negative Word Count'] = negative_count\n",
    "\n",
    "# Create a count vectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviewText'], df['Positive Word Count'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the vectorizer to the training data and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Predict the positive word counts for the test data\n",
    "predicted_counts = classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate accuracy score of the model\n",
    "accuracy = accuracy_score(y_test, predicted_counts)\n",
    "print(\"Accuracy of Naive Bayes model:\", accuracy)\n",
    "\n",
    "# Print the predicted positive word counts\n",
    "print(\"Predicted Positive Word Counts:\", predicted_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd5db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab61d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8097660223804679\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.43      0.53        54\n",
      "         2.0       0.00      0.00      0.00        14\n",
      "         3.0       0.00      0.00      0.00        25\n",
      "         4.0       0.31      0.05      0.08       110\n",
      "         5.0       0.83      0.98      0.90       780\n",
      "\n",
      "    accuracy                           0.81       983\n",
      "   macro avg       0.37      0.29      0.30       983\n",
      "weighted avg       0.73      0.81      0.75       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#unsing naive bays model in this N-gram feature and sentiment polarity to predict overall(rating or score)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "\n",
    "# Set the file path to your dataset\n",
    "file_path = r'C:\\Users\\nh013\\Desktop\\amazaon dataset\\amazon_reviews.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove any rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove any URLs\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# Remove any special characters\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Convert all text to lowercase\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Apply stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# Perform sentiment analysis using VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to generate N-grams\n",
    "def generate_ngrams(text, n):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    ngrams_list = list(ngrams(tokenized_text, n))\n",
    "    return [' '.join(grams) for grams in ngrams_list]\n",
    "\n",
    "# Add N-grams and sentiment polarity as new columns\n",
    "n = 2  # Set n as desired (e.g., n=2 for bigrams)\n",
    "df['ngrams'] = df['reviewText'].apply(lambda x: generate_ngrams(x, n=n))\n",
    "df['sentiment_polarity'] = df['reviewText'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Prepare data for Naive Bayes classification\n",
    "X = df['reviewText']\n",
    "y = df['overall']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy and performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc19099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbb164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0504f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost model: 0.8107833163784334\n",
      "Predicted Overall Ratings: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5\n",
      " 4 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 4 5 5 5 5 5 5 5 5 5 4 1 5 4 4 5 5 5\n",
      " 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 1 4 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 4 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 4\n",
      " 5 5 5 5 1 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 1 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 4 5 5 5 5 5 5 4 5 5 5 5 5 5 5 1 5 5 5 5\n",
      " 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 5 4 5 5 5 5 5 5 5 5 5 5 5 3 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 1 5 5 5 5 5 5 5 5 5 1 5 5 5 5\n",
      " 5 5 5 1 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 5 5 5 5 5 5 4 5 5 5 5 5 5 5\n",
      " 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 1 5 3 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 1 5 5 4 5 5 5 5 5\n",
      " 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 1 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 1 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 4 5]\n"
     ]
    }
   ],
   "source": [
    "# using XGBOOST model to predict overall rating in sentiment polarity and N-grams feature\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set the file path to your dataset\n",
    "file_path = r'C:\\Users\\nh013\\Desktop\\amazaon dataset\\amazon_reviews.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove any rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove any URLs\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# Remove any special characters\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Convert all text to lowercase\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Apply stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# Perform sentiment analysis using VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to generate N-grams\n",
    "def generate_ngrams(text, n):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    ngrams_list = list(ngrams(tokenized_text, n))\n",
    "    return [' '.join(grams) for grams in ngrams_list]\n",
    "\n",
    "# Add N-grams and sentiment polarity as new columns\n",
    "n = 2  # Set n as desired (e.g., n=2 for bigrams)\n",
    "df['ngrams'] = df['reviewText'].apply(lambda x: generate_ngrams(x, n=n))\n",
    "df['sentiment_polarity'] = df['reviewText'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Define features and target variable\n",
    "x = df['reviewText']\n",
    "y = df['overall']\n",
    "\n",
    "# Map target variable values to start from 0\n",
    "y_mapped = y - 1\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_mapped.astype(int), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a count vectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Transform the test data\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(x_train_vectorized, y_train)\n",
    "\n",
    "# Predict the overall ratings for the test data\n",
    "predicted_ratings = classifier.predict(x_test_vectorized)\n",
    "\n",
    "# Calculate accuracy score of the model\n",
    "accuracy = accuracy_score(y_test, predicted_ratings)\n",
    "print(\"Accuracy of XGBoost model:\", accuracy)\n",
    "\n",
    "\n",
    "# Map the predicted ratings back to the original scale\n",
    "predicted_ratings_mapped = predicted_ratings + 1\n",
    "\n",
    "# Print the predicted overall ratings\n",
    "print(\"Predicted Overall Ratings:\", predicted_ratings_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661ff51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
